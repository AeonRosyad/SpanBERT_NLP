{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AeonRosyad/SpanBERT_NLP/blob/main/SpanBERTNLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnF__G0IoxyQ"
      },
      "source": [
        "## Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOPNyjCeoTFQ",
        "outputId": "cd10f7a9-1cf4-4a9b-e093-e457da8ecf93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.37.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.37.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.2)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.11 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.1.0+cu121)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.27.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.11->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.11->transformers[torch]) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.11->transformers[torch]) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.11->transformers[torch]) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.11->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.11->transformers[torch]) (1.3.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.27.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm\n",
        "!pip install transformers\n",
        "!pip install transformers[torch]\n",
        "!pip install accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQGFbX-Coi2d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "import ast\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "from transformers import Adafactor, get_linear_schedule_with_warmup, AdamW\n",
        "from typing import Tuple\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import BertTokenizerFast, BertForTokenClassification\n",
        "from sklearn.metrics import precision_recall_fscore_support,accuracy_score\n",
        "from itertools import groupby\n",
        "from operator import itemgetter\n",
        "import ast\n",
        "from numpy import exp\n",
        "import importlib, sys\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRcPOctYo5tn"
      },
      "source": [
        "## Penggabungan Dataset DAMIR & DOSSPIRE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrS5rFeBow-5"
      },
      "outputs": [],
      "source": [
        "# Membaca dataset\n",
        "df = pd.read_excel('/content/DAMIR.xlsx')\n",
        "df_2 = pd.read_csv('/content/DOSSPRE 1.0Original.csv', encoding='latin1')\n",
        "df_3 = pd.read_csv('/content/software_requirements_extended.csv', encoding='latin1')\n",
        "data_1 = df['Context']\n",
        "data_2 = df_2['Requirement']\n",
        "data_3 = df_3['Requirement']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVV1QxdYpDWB"
      },
      "outputs": [],
      "source": [
        "# Menambahkan kolom 'Source' untuk membedakan sumber data\n",
        "data_1 = data_1.rename_axis('Text').reset_index()\n",
        "data_1['Source'] = 'DAMIR'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Es7gJL7EpDNi"
      },
      "outputs": [],
      "source": [
        "data_2 = data_2.rename_axis('Text').reset_index()\n",
        "data_2['Source'] = 'DOSSPRE'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLIIopGbFBaZ"
      },
      "outputs": [],
      "source": [
        "data_3 = data_3.rename_axis('Text').reset_index()\n",
        "data_3['Source'] = 'KAGGLETYPE'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YPldnknpGf3"
      },
      "outputs": [],
      "source": [
        "# Langkah 1: Gabungkan dataset\n",
        "df = pd.concat([data_1, data_2, data_3], axis=0, ignore_index=True)\n",
        "\n",
        "# Langkah 2: Simpan hasil penggabungan ke dalam CSV\n",
        "df.to_csv('dataset.csv', index=False)\n",
        "\n",
        "dft = pd.read_csv('dataset.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "PSRBzOazpJ3h",
        "outputId": "678ef5b4-5e7e-4dfd-b617-4e555688b6c1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          Source                                        Requirement\n",
              "0          DAMIR  Under the Halifax-class WPC-W and WPC-E, DND w...\n",
              "1          DAMIR  Under the Halifax-class WPC-W and WPC-E, DND w...\n",
              "2          DAMIR  Under the Halifax-class WPC-W and WPC-E, DND w...\n",
              "3          DAMIR  Under the Halifax-class WPC-W and WPC-E, DND w...\n",
              "4          DAMIR  Under the Halifax-class WPC-W and WPC-E, DND w...\n",
              "...          ...                                                ...\n",
              "7793  KAGGLETYPE  There will be a designated phone number that u...\n",
              "7794  KAGGLETYPE  Texts sent to that number will be sent to the ...\n",
              "7795  KAGGLETYPE  If a question is not understood by our API, th...\n",
              "7796  KAGGLETYPE  Upon the USB being plugged in the system shall...\n",
              "7797  KAGGLETYPE  The system shall be able to handle 1000 custom...\n",
              "\n",
              "[7798 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ebed8ddb-0985-46bb-bbf4-0aec6f9e6307\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Source</th>\n",
              "      <th>Requirement</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>DAMIR</td>\n",
              "      <td>Under the Halifax-class WPC-W and WPC-E, DND w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>DAMIR</td>\n",
              "      <td>Under the Halifax-class WPC-W and WPC-E, DND w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>DAMIR</td>\n",
              "      <td>Under the Halifax-class WPC-W and WPC-E, DND w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>DAMIR</td>\n",
              "      <td>Under the Halifax-class WPC-W and WPC-E, DND w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>DAMIR</td>\n",
              "      <td>Under the Halifax-class WPC-W and WPC-E, DND w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7793</th>\n",
              "      <td>KAGGLETYPE</td>\n",
              "      <td>There will be a designated phone number that u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7794</th>\n",
              "      <td>KAGGLETYPE</td>\n",
              "      <td>Texts sent to that number will be sent to the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7795</th>\n",
              "      <td>KAGGLETYPE</td>\n",
              "      <td>If a question is not understood by our API, th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7796</th>\n",
              "      <td>KAGGLETYPE</td>\n",
              "      <td>Upon the USB being plugged in the system shall...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7797</th>\n",
              "      <td>KAGGLETYPE</td>\n",
              "      <td>The system shall be able to handle 1000 custom...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7798 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ebed8ddb-0985-46bb-bbf4-0aec6f9e6307')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ebed8ddb-0985-46bb-bbf4-0aec6f9e6307 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ebed8ddb-0985-46bb-bbf4-0aec6f9e6307');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1aa531d5-101d-41c5-b44a-c5cc7a36c78d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1aa531d5-101d-41c5-b44a-c5cc7a36c78d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1aa531d5-101d-41c5-b44a-c5cc7a36c78d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "dft",
              "summary": "{\n  \"name\": \"dft\",\n  \"rows\": 7798,\n  \"fields\": [\n    {\n      \"column\": \"Source\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"DAMIR\",\n          \"DOSSPRE\",\n          \"KAGGLETYPE\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Requirement\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2830,\n        \"samples\": [\n          \"'The system shall be able to notify the respected parties of the clinical incident  even when there is a fault '\",\n          \"'The system shall meet user\\u0092s needs regarding ease of entry, ease of learning, ease of handling, likability, and possible metrics.  Usability'\",\n          \"The system shall have high availability every day of the year.The system must be available for use between 12\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "# Mengisi nilai NaN di kolom 'Requirement' dengan nilai dari kolom 'Context'\n",
        "dft['Requirement'].fillna(dft['Context'], inplace=True)\n",
        "dft = dft.drop(columns=['Context', 'Text'])\n",
        "dft\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqZE-y1apNGo"
      },
      "source": [
        "## Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "sDjobEAHpQYV",
        "outputId": "7e753d01-e6f8-463d-baee-8dae88d13e7a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          Source                                        Requirement\n",
              "0          DAMIR  Under the Halifax-class WPC-W and WPC-E, DND w...\n",
              "1          DAMIR  Canada will use other organizations to support...\n",
              "2          DAMIR  The ship readiness levels and expected annual ...\n",
              "3          DAMIR  HMC Ships on the West Coast are assigned to Ma...\n",
              "4          DAMIR  The MARLANT and MARPAC Deputy Chiefs of Staff ...\n",
              "...          ...                                                ...\n",
              "2825  KAGGLETYPE  There will be a designated phone number that u...\n",
              "2826  KAGGLETYPE  Texts sent to that number will be sent to the ...\n",
              "2827  KAGGLETYPE  If a question is not understood by our API, th...\n",
              "2828  KAGGLETYPE  Upon the USB being plugged in the system shall...\n",
              "2829  KAGGLETYPE  The system shall be able to handle 1000 custom...\n",
              "\n",
              "[2830 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6dd0ccaa-8416-4cd9-b2d7-8be53046f26b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Source</th>\n",
              "      <th>Requirement</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>DAMIR</td>\n",
              "      <td>Under the Halifax-class WPC-W and WPC-E, DND w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>DAMIR</td>\n",
              "      <td>Canada will use other organizations to support...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>DAMIR</td>\n",
              "      <td>The ship readiness levels and expected annual ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>DAMIR</td>\n",
              "      <td>HMC Ships on the West Coast are assigned to Ma...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>DAMIR</td>\n",
              "      <td>The MARLANT and MARPAC Deputy Chiefs of Staff ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2825</th>\n",
              "      <td>KAGGLETYPE</td>\n",
              "      <td>There will be a designated phone number that u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2826</th>\n",
              "      <td>KAGGLETYPE</td>\n",
              "      <td>Texts sent to that number will be sent to the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2827</th>\n",
              "      <td>KAGGLETYPE</td>\n",
              "      <td>If a question is not understood by our API, th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2828</th>\n",
              "      <td>KAGGLETYPE</td>\n",
              "      <td>Upon the USB being plugged in the system shall...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2829</th>\n",
              "      <td>KAGGLETYPE</td>\n",
              "      <td>The system shall be able to handle 1000 custom...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2830 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6dd0ccaa-8416-4cd9-b2d7-8be53046f26b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6dd0ccaa-8416-4cd9-b2d7-8be53046f26b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6dd0ccaa-8416-4cd9-b2d7-8be53046f26b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-cb11d3cc-f9aa-4c8b-9851-3eda9943ba68\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cb11d3cc-f9aa-4c8b-9851-3eda9943ba68')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-cb11d3cc-f9aa-4c8b-9851-3eda9943ba68 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 2830,\n  \"fields\": [\n    {\n      \"column\": \"Source\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"DAMIR\",\n          \"DOSSPRE\",\n          \"KAGGLETYPE\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Requirement\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2830,\n        \"samples\": [\n          \"'The system shall be able to notify the respected parties of the clinical incident  even when there is a fault '\",\n          \"'The system shall meet user\\u0092s needs regarding ease of entry, ease of learning, ease of handling, likability, and possible metrics.  Usability'\",\n          \"The system shall have high availability every day of the year.The system must be available for use between 12\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "# Mengahpus Duplikat\n",
        "data = dft.drop_duplicates()\n",
        "data = data.reset_index(drop=True)\n",
        "\n",
        "# Menghapus baris dengan missing values\n",
        "data = data.dropna()\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3c7q48UnjyC"
      },
      "source": [
        "#Deteksi Anaforis Kata Ganti Ambigu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMvu91eCTEPD",
        "outputId": "33171a35-471b-4ee8-9cf3-ec69a973b8ad"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       Under the Halifax-class WPC-W and WPC-E, DND w...\n",
              "1       Canada will use other organizations to support...\n",
              "2       The ship readiness levels and expected annual ...\n",
              "3       HMC Ships on the West Coast are assigned to Ma...\n",
              "4       The MARLANT and MARPAC Deputy Chiefs of Staff ...\n",
              "                              ...                        \n",
              "2825    There will be a designated phone number that u...\n",
              "2826    Texts sent to that number will be sent to the ...\n",
              "2827    If a question is not understood by our API, th...\n",
              "2828    Upon the USB being plugged in the system shall...\n",
              "2829    The system shall be able to handle 1000 custom...\n",
              "Name: Context, Length: 2830, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "df = data.rename(columns={'Requirement': 'Context'})\n",
        "exampleData = df['Context']\n",
        "exampleData"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQYosKR7InZ8"
      },
      "source": [
        "## Processing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCng68G1TYzV",
        "outputId": "4df62a9b-86a6-474a-d330-9bb64fa5b9c6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       (Under, the, Halifax, -, class, WPC, -, W, and...\n",
              "1       (Canada, will, use, other, organizations, to, ...\n",
              "2       (The, ship, readiness, levels, and, expected, ...\n",
              "3       (HMC, Ships, on, the, West, Coast, are, assign...\n",
              "4       (The, MARLANT, and, MARPAC, Deputy, Chiefs, of...\n",
              "                              ...                        \n",
              "2825    (There, will, be, a, designated, phone, number...\n",
              "2826    (Texts, sent, to, that, number, will, be, sent...\n",
              "2827    (If, a, question, is, not, understood, by, our...\n",
              "2828    (Upon, the, USB, being, plugged, in, the, syst...\n",
              "2829    (The, system, shall, be, able, to, handle, 100...\n",
              "Name: Context, Length: 2830, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def applynlp(string,nlp):\n",
        "    tr=np.nan\n",
        "    try:\n",
        "        tr=nlp(string)\n",
        "    except:\n",
        "        print(string)\n",
        "    return tr\n",
        "\n",
        "exampleData= exampleData.apply(lambda x: applynlp(x,nlp))\n",
        "exampleData"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "ctCPHr_GUncQ",
        "outputId": "0e0406d2-4791-471d-fcb2-b2f940faa16e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                       Id                                            Context  \\\n",
              "0                 0-its-0  (Under, the, Halifax, -, class, WPC, -, W, and...   \n",
              "1               1-their-0  (Canada, will, use, other, organizations, to, ...   \n",
              "2                 2-its-0  (The, ship, readiness, levels, and, expected, ...   \n",
              "3               3-their-0  (HMC, Ships, on, the, West, Coast, are, assign...   \n",
              "4               4-their-0  (The, MARLANT, and, MARPAC, Deputy, Chiefs, of...   \n",
              "...                   ...                                                ...   \n",
              "2825  2825-NoPronoun-1844  (There, will, be, a, designated, phone, number...   \n",
              "2826  2826-NoPronoun-1845  (Texts, sent, to, that, number, will, be, sent...   \n",
              "2827        2827-our-1846  (If, a, question, is, not, understood, by, our...   \n",
              "2828  2828-NoPronoun-1846  (Upon, the, USB, being, plugged, in, the, syst...   \n",
              "2829  2829-NoPronoun-1847  (The, system, shall, be, able, to, handle, 100...   \n",
              "\n",
              "     Pronoun  Position                               Candidate Antecedent  \n",
              "0        its      77.0  (the, Halifax, -, class, WPC, -, W, and, WPC, ...  \n",
              "1      their      60.0                                           (Canada)  \n",
              "2        its      32.0                     (The, ship, readiness, levels)  \n",
              "3      their      30.0                                       (HMC, Ships)  \n",
              "4      their      14.0                                     (The, MARLANT)  \n",
              "...      ...       ...                                                ...  \n",
              "2825    None       NaN                                               None  \n",
              "2826    None       NaN                                               None  \n",
              "2827     our       7.0                                      (a, question)  \n",
              "2828    None       NaN                                               None  \n",
              "2829    None       NaN                                               None  \n",
              "\n",
              "[2830 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f318a530-2aec-47fe-a1ac-bc0598cc8e4c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>Context</th>\n",
              "      <th>Pronoun</th>\n",
              "      <th>Position</th>\n",
              "      <th>Candidate Antecedent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0-its-0</td>\n",
              "      <td>(Under, the, Halifax, -, class, WPC, -, W, and...</td>\n",
              "      <td>its</td>\n",
              "      <td>77.0</td>\n",
              "      <td>(the, Halifax, -, class, WPC, -, W, and, WPC, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1-their-0</td>\n",
              "      <td>(Canada, will, use, other, organizations, to, ...</td>\n",
              "      <td>their</td>\n",
              "      <td>60.0</td>\n",
              "      <td>(Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2-its-0</td>\n",
              "      <td>(The, ship, readiness, levels, and, expected, ...</td>\n",
              "      <td>its</td>\n",
              "      <td>32.0</td>\n",
              "      <td>(The, ship, readiness, levels)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3-their-0</td>\n",
              "      <td>(HMC, Ships, on, the, West, Coast, are, assign...</td>\n",
              "      <td>their</td>\n",
              "      <td>30.0</td>\n",
              "      <td>(HMC, Ships)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4-their-0</td>\n",
              "      <td>(The, MARLANT, and, MARPAC, Deputy, Chiefs, of...</td>\n",
              "      <td>their</td>\n",
              "      <td>14.0</td>\n",
              "      <td>(The, MARLANT)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2825</th>\n",
              "      <td>2825-NoPronoun-1844</td>\n",
              "      <td>(There, will, be, a, designated, phone, number...</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2826</th>\n",
              "      <td>2826-NoPronoun-1845</td>\n",
              "      <td>(Texts, sent, to, that, number, will, be, sent...</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2827</th>\n",
              "      <td>2827-our-1846</td>\n",
              "      <td>(If, a, question, is, not, understood, by, our...</td>\n",
              "      <td>our</td>\n",
              "      <td>7.0</td>\n",
              "      <td>(a, question)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2828</th>\n",
              "      <td>2828-NoPronoun-1846</td>\n",
              "      <td>(Upon, the, USB, being, plugged, in, the, syst...</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2829</th>\n",
              "      <td>2829-NoPronoun-1847</td>\n",
              "      <td>(The, system, shall, be, able, to, handle, 100...</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2830 rows × 5 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f318a530-2aec-47fe-a1ac-bc0598cc8e4c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f318a530-2aec-47fe-a1ac-bc0598cc8e4c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f318a530-2aec-47fe-a1ac-bc0598cc8e4c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c02f2dad-3fc5-49f9-9322-b5648332f12a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c02f2dad-3fc5-49f9-9322-b5648332f12a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c02f2dad-3fc5-49f9-9322-b5648332f12a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "exampleDataProcessed",
              "summary": "{\n  \"name\": \"exampleDataProcessed\",\n  \"rows\": 2830,\n  \"fields\": [\n    {\n      \"column\": \"Id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2830,\n        \"samples\": [\n          \"1025-NoPronoun-388\",\n          \"1413-NoPronoun-678\",\n          \"2238-NoPronoun-1376\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Context\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2830,\n        \"samples\": [\n          \"'The system shall be able to notify the respected parties of the clinical incident  even when there is a fault '\",\n          \"'The system shall meet user\\u0092s needs regarding ease of entry, ease of learning, ease of handling, likability, and possible metrics.  Usability'\",\n          \"The system shall have high availability every day of the year.The system must be available for use between 12\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Pronoun\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 982,\n        \"samples\": [\n          \"it\",\n          \"its\",\n          \"himself\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Position\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 19.357159148876274,\n        \"min\": 0.0,\n        \"max\": 107.0,\n        \"num_unique_values\": 88,\n        \"samples\": [\n          65.0,\n          77.0,\n          47.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Candidate Antecedent\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 913,\n        \"samples\": [\n          \"numerical measurements\",\n          \"User\",\n          \"The IETIS\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "pronouns=[\"I\",\"me\",\"my\",\"mine\",\"myself\",\"you\",\"your\",\"yours\",\"yourself\",\"he\",\"him\",\"his\",\"himself\",\"she\",\"her\",\"hers\",\"herself\",\"it\",\"its\",\"itself\",\"we\",\"us\",\"our\",\"ours\",\"ourselves\",\"they\",\"them\",\"their\",\"theirs\",\"themselves\"]\n",
        "li=[]\n",
        "i,j=0,0\n",
        "ids=[]\n",
        "\n",
        "def findPronouns(sent, pronouns, nlp):\n",
        "    # Menganalisis kalimat menjadi objek token\n",
        "    doc = nlp(sent)\n",
        "    tokens = []\n",
        "\n",
        "    for t in doc:\n",
        "        if \"PRP\" in t.tag_ and t.text.lower() in pronouns and t not in tokens:\n",
        "            tokens.append(t)\n",
        "\n",
        "    return tokens\n",
        "\n",
        "def getNPs(sent,p,include_nouns=False):\n",
        "    nps=[]\n",
        "    npstr=[]\n",
        "    chunks = list(sent.noun_chunks)\n",
        "    for i in range(len(chunks)):\n",
        "        np=chunks[i]\n",
        "        if np.end<=p.i:\n",
        "            if len(np)==1:\n",
        "                if np[0].pos_ not in [\"NOUN\",\"PROPN\"]:\n",
        "                    continue\n",
        "            if np.text.lower() in npstr:\n",
        "                for x in nps:\n",
        "                    if x.text.lower() == np.text.lower():\n",
        "                        nps.remove(x)\n",
        "                npstr.remove(np.text.lower())\n",
        "            nps.append(np)\n",
        "            npstr.append(np.text.lower())\n",
        "            if i < len(chunks)-1:\n",
        "                np1=chunks[i+1]\n",
        "                if np1.start-np.end==1:\n",
        "                    if sent.doc[np.end].tag_==\"CC\":\n",
        "                        newnp = sent.doc[np.start:np1.end]\n",
        "                        if newnp.text.lower() in npstr:\n",
        "                            for x in nps:\n",
        "                                if x.text.lower() == newnp.text.lower():\n",
        "                                    nps.remove(x)\n",
        "                            npstr.remove(newnp.text.lower())\n",
        "                        nps.append(newnp)\n",
        "                        npstr.append(newnp.text.lower())\n",
        "\n",
        "    if include_nouns:\n",
        "        for t in sent:\n",
        "            if t.i<p.i and \"subj\" in t.dep_ and t.pos_==\"NOUN\": # to revisit\n",
        "                if t.text.lower() in npstr:\n",
        "                    for x in nps:\n",
        "                        if x.text.lower() == t.text.lower():\n",
        "                            nps.remove(x)\n",
        "                    npstr.remove(t.text.lower())\n",
        "                npstr.append(t.text.lower())\n",
        "                nps.append(sent[t.i:t.i+1])\n",
        "    return nps\n",
        "\n",
        "for context in exampleData:\n",
        "    pronoun = findPronouns(context, pronouns, nlp)  # Mengubah loop untuk hanya satu kata ganti\n",
        "    if pronoun:\n",
        "        pronoun = pronoun[0]  # Mengambil hanya satu kata ganti\n",
        "        Id = str(i)+\"-\"+pronoun.text+\"-\"+str(j)\n",
        "        while Id in ids:\n",
        "            j+=1\n",
        "            Id = str(i)+\"-\"+pronoun.text+\"-\"+str(j)\n",
        "\n",
        "        candidateAntecedent = getNPs(context, pronoun)  # Mengambil kandidat antecedent\n",
        "        if candidateAntecedent:\n",
        "            candidateAntecedent = candidateAntecedent[0]  # Mengambil hanya satu kandidat antecedent\n",
        "\n",
        "            li.append([Id, context, pronoun, pronoun.i, candidateAntecedent])\n",
        "            ids.append(Id)\n",
        "        else:  # Jika tidak ada kandidat antecedent\n",
        "            li.append([Id, context, pronoun, pronoun.i, None])\n",
        "            ids.append(Id)\n",
        "    else:  # Jika tidak ada kata ganti dalam kalimat\n",
        "        li.append([str(i) + \"-NoPronoun-\" + str(j), context, None, None, None])\n",
        "        j += 1\n",
        "\n",
        "    i += 1\n",
        "\n",
        "exampleDataProcessed = pd.DataFrame(li, columns=[\"Id\",\"Context\",\"Pronoun\",\"Position\",\"Candidate Antecedent\"])\n",
        "exampleDataProcessed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCvT_Wsvd3OR",
        "outputId": "9d59053c-ec57-4c57-ecfd-26408fd551e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "3GLw9nKOxM_z",
        "outputId": "5fb3b8df-c1bb-45cf-eaa1-409af46e90e9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "Can't load tokenizer for 'SpanBERT/spanbert-base-cased'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'SpanBERT/spanbert-base-cased' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-146faa8f94ac>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfast_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizerFast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SpanBERT/spanbert-base-cased'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel_path_nlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/SpanBERT-NLPv21.8.10\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_path_re\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/SpanBERT-REv21.9.01\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnlpmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForTokenClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path_nlp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2011\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_file_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfull_file_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresolved_vocab_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2013\u001b[0;31m             raise EnvironmentError(\n\u001b[0m\u001b[1;32m   2014\u001b[0m                 \u001b[0;34mf\"Can't load tokenizer for '{pretrained_model_name_or_path}'. If you were trying to load it from \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2015\u001b[0m                 \u001b[0;34m\"'https://huggingface.co/models', make sure you don't have a local directory with the same name. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for 'SpanBERT/spanbert-base-cased'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'SpanBERT/spanbert-base-cased' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer."
          ]
        }
      ],
      "source": [
        "fast_tokenizer = BertTokenizerFast.from_pretrained('SpanBERT/spanbert-base-cased')\n",
        "model_path_nlp = \"/content/drive/MyDrive/SpanBERT-NLPv21.8.10\"\n",
        "model_path_re = \"/content/drive/MyDrive/SpanBERT-REv21.9.01\"\n",
        "\n",
        "nlpmodel = BertForTokenClassification.from_pretrained(model_path_nlp)\n",
        "remodel = BertForTokenClassification.from_pretrained(model_path_re)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2Knbii0x9b7"
      },
      "outputs": [],
      "source": [
        "def get_tokenclassification_annotation(item, fast_tokenizer, train=True, debug=False,max_length=128):\n",
        "  text = item['context']+\" [SEP] \"+item['pronoun']\n",
        "  tokens_info = fast_tokenizer(text, padding='max_length', max_length=max_length, truncation=True, return_offsets_mapping = True)\n",
        "  # padding='max_length', max_length=max_length, truncation=True,\n",
        "  if debug:\n",
        "    print(f'Original Text:\\n\\t {text}')\n",
        "  if train:\n",
        "    spans = transform_strtab_inttab(item['schars'])\n",
        "    chunks = compute_chunks(spans)\n",
        "    if debug:\n",
        "      print(\"Antecedent text\")\n",
        "      for chunk in chunks:\n",
        "        print('\\t',text[chunk[0]:chunk[-1]])\n",
        "    tokenized_text = fast_tokenizer.convert_ids_to_tokens(tokens_info['input_ids'])\n",
        "    if debug:\n",
        "      print(f'Tokenized Text:\\n\\t {tokenized_text}')\n",
        "    offsets_mapping = tokens_info['offset_mapping']\n",
        "    annot = []\n",
        "    for k in spans:\n",
        "      for x, (j, v) in enumerate(offsets_mapping[1:-1]):\n",
        "        if j <= k and k < v  :\n",
        "          annot.append(x)\n",
        "    annot = set(annot)\n",
        "    annotation = [0 for x in  offsets_mapping[1:-1]]\n",
        "    if debug:\n",
        "      print(\"Antecedent words\")\n",
        "    for x in annot:\n",
        "      annotation[x] = 1\n",
        "      if debug:\n",
        "        print('\\t',tokenized_text[1:-1][x], \"||||index: \", x)\n",
        "    return {'input_ids':tokens_info['input_ids'],\n",
        "            'labels':[0] + annotation + [0],\n",
        "            'attention_mask': tokens_info['attention_mask']\n",
        "    }\n",
        "  return {'input_ids':tokens_info['input_ids'],\n",
        "            'attention_mask': tokens_info['attention_mask']\n",
        "    }\n",
        "\n",
        "def transform_strtab_inttab(s):\n",
        "  s=str(s)\n",
        "  return [int(x) for x in s[1:-1].split(',') if len(x)>0]\n",
        "\n",
        "def compute_chunks(tab):\n",
        "  init_chunck  = 0\n",
        "  chunks = []\n",
        "  for i, k in enumerate(tab[:-1]):\n",
        "    if k+1 != tab[i+1]:\n",
        "      chunks.append(tab[init_chunck: i+1])\n",
        "      init_chunck = i+1\n",
        "  chunks.append(tab[init_chunck:])\n",
        "  return chunks\n",
        "\n",
        "\n",
        "class SpanDetectionData(Dataset):\n",
        "  def __init__(self, df, tokenizer, train=True):\n",
        "    self.df = df\n",
        "    self.tokenizer = tokenizer\n",
        "    self.train = train\n",
        "\n",
        "  def __len__(self,):\n",
        "    return len(self.df)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    item = get_tokenclassification_annotation(self.df.iloc[idx], self.tokenizer, self.train)\n",
        "    return item\n",
        "\n",
        "test=[]\n",
        "test_no_pronoun=[]  # DataFrame untuk kalimat tanpa kata ganti\n",
        "\n",
        "for Id in exampleDataProcessed.Id.unique():\n",
        "    c = exampleDataProcessed[exampleDataProcessed.Id==Id].Context.unique()[0]\n",
        "    pronoun = exampleDataProcessed[exampleDataProcessed.Id==Id].Pronoun.unique()[0]\n",
        "\n",
        "    if c is not None and pronoun is not None:  # Memeriksa apakah c dan pronoun tidak None\n",
        "        hashedpronoun=pronoun.text+\"#1\"\n",
        "        hashedcontext=c[:pronoun.i].text+\" \"+hashedpronoun+\" \"+c[pronoun.i+1:].text\n",
        "        test.append([Id,hashedcontext,hashedpronoun])\n",
        "    else:\n",
        "        hashedcontext = \"\"  # Kalimat tanpa kata ganti akan diabaikan\n",
        "        test_no_pronoun.append([Id, c, pronoun])  # Menyimpan kalimat tanpa kata ganti\n",
        "\n",
        "testdf=pd.DataFrame(test,columns=[\"Id\",\"context\",\"pronoun\"])\n",
        "test_data = SpanDetectionData(testdf, fast_tokenizer,train=False)\n",
        "\n",
        "testdf_no_pronoun = pd.DataFrame(test_no_pronoun, columns=[\"Id\",\"context\",\"pronoun\"])\n",
        "test_data_no_pronoun = SpanDetectionData(testdf_no_pronoun, fast_tokenizer, train=False)\n",
        "testdf_no_pronoun['context'] = testdf_no_pronoun['context'].apply(lambda x: x.text if x is not None else \"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcRRbK6J0gPI"
      },
      "source": [
        "### SpanBERT Solution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpK9eI19x_Z2"
      },
      "outputs": [],
      "source": [
        "for param in nlpmodel.base_model.parameters():    # semua parameter dalam model nlpmodel menjadi False\n",
        "    param.requires_grad = False       # parameter-parameter ini tidak akan mengalami pembelajaran (learning) selama proses training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHcada9K0qgK"
      },
      "outputs": [],
      "source": [
        "# Mengatur Argumentasi Pelatihan\n",
        "training_args = TrainingArguments(             # parameter dan konfigurasi yang diperlukan untuk melatih model\n",
        "    output_dir='./results',                    # Direktori tempat hasil training akan disimpan.\n",
        "    eval_steps=200,\n",
        "    logging_steps=200,                         # frekuensi logging selama training (setiap 200 iterasi).\n",
        "    save_steps=200,                            # Frekuensi penyimpanan checkpoint selama training (setiap 200 iterasi).\n",
        "    save_total_limit=5,                        # Jumlah checkpoint yang akan disimpan secara keseluruhan (maksimal 5).\n",
        "    num_train_epochs=4,                        # Jumlah epoch yang akan dilatih (4 epoch)\n",
        "    # learning_rate=2e-5,                      # Tingkat pembelajaran (learning rate) yang digunakan (2e-5).\n",
        "    per_device_train_batch_size=32,            # Jumlah sampel yang digunakan per batch selama training (32 sampel per perangkat).\n",
        "    per_device_eval_batch_size=32,             # Jumlah sampel yang digunakan per batch selama evaluasi (32 sampel per perangkat).\n",
        "    warmup_steps=500,                          # Jumlah langkah \"pemanasan\" awal selama training.\n",
        "    weight_decay=0.01,                         # Pengaturan penalti untuk regularisasi L2.\n",
        "    logging_dir='./logs',                      # Direktori tempat log dari training akan disimpan.\n",
        "    report_to='tensorboard',\n",
        "    run_name='span_detection'\n",
        ")\n",
        "\n",
        "# Inisiasi Trainer\n",
        "nlp_trainer = Trainer(                         # inisialisasi sebagai Trainer yang akan digunakan untuk melatih model\n",
        "    model=nlpmodel,                            # Model yang akan dilatih\n",
        "    args=training_args,                        # konfigurasi pelatihan\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-1iAO3g-s5n"
      },
      "outputs": [],
      "source": [
        "def continuousSubArrays(L):\n",
        "  sas=[]\n",
        "  sa=[]\n",
        "  for i,j in zip(range(0,len(L)-1),range(1,len(L))):\n",
        "          if L[i]==L[j]-1 or L[i]==L[j]-2:\n",
        "              if L[i] not in sa:\n",
        "                  sa.append(L[i])\n",
        "              if L[j] not in sa:\n",
        "                  sa.append(L[j])\n",
        "          else:\n",
        "              if sa:\n",
        "                  sas.append(sa)\n",
        "                  sa=[]\n",
        "  if sa:\n",
        "      sas.append(sa)\n",
        "  return sas\n",
        "\n",
        "def softmax(vector):\n",
        "  li=[]\n",
        "  for element in vector:\n",
        "    se=exp(element).sum()\n",
        "    ne=exp(element)/se\n",
        "    li.append(ne)\n",
        "  return np.array(li)\n",
        "\n",
        "def predict_spans(truncated_predictions, cdf,fast_tokenizer,T=0):\n",
        "  predited_spans = []\n",
        "  mean=-sys.maxsize\n",
        "  for i in range(len(cdf)):\n",
        "    span=[]\n",
        "    nmean=-sys.maxsize-1\n",
        "    aboveT=0\n",
        "    for x in continuousSubArrays(np.where(np.argmax(truncated_predictions[i],1)==1)[0]):\n",
        "        if softmax(truncated_predictions[i])[:,1][x].mean()>nmean:\n",
        "            nmean=softmax(truncated_predictions[i])[:,1][x].mean()\n",
        "            if nmean>T:\n",
        "              aboveT+=1\n",
        "            span=(np.array(x, dtype=np.int64),)\n",
        "    #nmean=softmax(truncated_predictions[i])[:,1][span].mean()\n",
        "    if T!=0:\n",
        "        if nmean<T:\n",
        "            predited_spans.append([])\n",
        "            continue\n",
        "    if aboveT>1:\n",
        "      predited_spans.append([])\n",
        "      continue\n",
        "    text = cdf.iloc[i]['context']\n",
        "    #print(text)\n",
        "    #discminative_preds = np.argmax(truncated_predictions[i],1)\n",
        "    #print(len(discminative_preds))\n",
        "    one_idx = span#np.where(discminative_preds == 1)\n",
        "    one_idx = one_idx[0]\n",
        "    tokens_info = fast_tokenizer(text, return_offsets_mapping=True)\n",
        "    # print(len(tokens_info['input_ids']))\n",
        "    # print(len(tokens_info['offset_mapping']))\n",
        "    #print(\"************************\")\n",
        "    yoyo = [tokens_info['offset_mapping'][k] for k in one_idx if k<len(tokens_info['offset_mapping'])]\n",
        "    pred = []\n",
        "    for p in yoyo:\n",
        "      pred += list(range(p[0],p[1]))\n",
        "    predited_spans.append(pred)\n",
        "  return predited_spans\n",
        "\n",
        "def processPred(predictions,train_data,df,fast_tokenizer,T=0.9):\n",
        "    truncated_predictions = []\n",
        "    for k in range(len(train_data)):\n",
        "      inputs_ids = train_data[k]['input_ids']\n",
        "      truncated_predictions.append(predictions.predictions[k][:len(inputs_ids)])\n",
        "    predicted_spans = predict_spans(truncated_predictions, df,fast_tokenizer,T)\n",
        "    return truncated_predictions,predicted_spans\n",
        "\n",
        "# Melakukan Prediksi dengan Model\n",
        "nlp_predictions = nlp_trainer.predict(test_data)\n",
        "\n",
        "# Proses Prediksi:\n",
        "ttruncated_predictions,tpredicted_spans=processPred(nlp_predictions,test_data,testdf,fast_tokenizer,T=0.9)    # Hasil prediksi dari model diproses menggunakan fungsi processPred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEQz75pCAioY"
      },
      "outputs": [],
      "source": [
        "def findspans(s,ids):\n",
        "    spans=[]\n",
        "    lastspan=\"\"\n",
        "    previ=-1\n",
        "    for i in ids:\n",
        "      i=int(i)\n",
        "      if previ==-1 or i==previ+1:\n",
        "          lastspan+=s[i]\n",
        "          previ=i\n",
        "          continue\n",
        "      elif i==previ+2:\n",
        "          lastspan+=\" \"+s[i]\n",
        "          previ=i\n",
        "          continue\n",
        "      else:\n",
        "          if lastspan not in spans:\n",
        "              if lastspan:\n",
        "                spans.append(lastspan)\n",
        "          lastspan=s[i]\n",
        "          previ=i\n",
        "          continue\n",
        "    if lastspan not in spans:\n",
        "        if lastspan:\n",
        "          spans.append(lastspan)\n",
        "    return spans\n",
        "\n",
        "spans=[]\n",
        "for i,j in zip(testdf.index, tpredicted_spans):\n",
        "    spans.append(findspans(testdf.context[i],j))\n",
        "testdf['Resolution']=spans\n",
        "testdf['Detection']=testdf['Resolution'].apply(lambda x: \"Unambiguous\" if len(x)!=0 else \"Ambiguous\")\n",
        "testdf\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PC7b2ajA05x"
      },
      "source": [
        "##Hasil Penanganan Ambiguitas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUCeSOqBAzrN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Buat DataFrame baru untuk menyimpan hasil\n",
        "output_data = []\n",
        "\n",
        "for i in testdf.index:\n",
        "    id = testdf['Id'][i]\n",
        "    context = testdf['context'][i]\n",
        "    pronoun = testdf['pronoun'][i]\n",
        "    antecedent = testdf['Resolution'][i]\n",
        "    detected_as = testdf['Detection'][i]\n",
        "\n",
        "    # Tambahkan baris baru ke DataFrame\n",
        "    output_data.append({'Id': id, 'Context': context, 'Pronouns': pronoun, 'Antecedent': antecedent, 'Pronouns_Ambiguous': detected_as})\n",
        "\n",
        "# Buat DataFrame dari list data\n",
        "output_df = pd.DataFrame(output_data)\n",
        "output_df['Context'] = output_df['Context'].str.replace('#1', '', regex=False)\n",
        "\n",
        "# Cetak DataFrame\n",
        "output_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAquPwrSv5E1"
      },
      "outputs": [],
      "source": [
        "# Langkah 1: Gabungkan dataset\n",
        "df = pd.concat([output_df, testdf_no_pronoun], axis=0, ignore_index=True)\n",
        "\n",
        "# Langkah 2: Simpan hasil penggabungan ke dalam CSV\n",
        "df.to_csv('pronouns_ambiguous.csv', index=False)\n",
        "\n",
        "pronoun_ambigu = pd.read_csv('pronouns_ambiguous.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0364Z8Sakq-"
      },
      "outputs": [],
      "source": [
        "# Mengisi nilai NaN di kolom 'Requirement' dengan nilai dari kolom 'Context'\n",
        "pronoun_ambigu['Context'].fillna(pronoun_ambigu['context'], inplace=True)\n",
        "pronoun_ambigu = pronoun_ambigu.drop(columns=['context','pronoun'])\n",
        "pronoun_ambigu = pronoun_ambigu.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UIoDHq0Lw2q"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Misalkan Anda memiliki DataFrame 'data' yang berisi data 'Requirement' dan 'Source'\n",
        "# 'pronoun_ambigu' adalah DataFrame yang sudah memiliki kolom 'Context' dan 'Pronouns_Ambiguous'\n",
        "\n",
        "def find_matching_source(context, dft):\n",
        "    # Memisahkan kata-kata dari 'Context' menjadi token\n",
        "    context_tokens = set(context.split())\n",
        "\n",
        "    # Mencari Requirement dengan token terbanyak yang cocok\n",
        "    best_matching_requirement = None\n",
        "    max_matching_tokens = 0\n",
        "\n",
        "    for requirement in dft['Requirement']:\n",
        "        requirement_tokens = set(requirement.split())\n",
        "        matching_tokens = len(context_tokens.intersection(requirement_tokens))\n",
        "\n",
        "        # Menyimpan nilai 'Source' jika memiliki token yang lebih banyak cocok\n",
        "        if matching_tokens > max_matching_tokens:\n",
        "            max_matching_tokens = matching_tokens\n",
        "            best_matching_requirement = requirement\n",
        "\n",
        "    # Mengambil nilai 'Source' berdasarkan Requirement dengan token terbanyak yang cocok\n",
        "    if best_matching_requirement:\n",
        "        matching_sources = dft[dft['Requirement'] == best_matching_requirement]['Source']\n",
        "        return matching_sources.mode()[0] if len(matching_sources) > 0 else None\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "pronoun_ambigu['Source'] = pronoun_ambigu['Context'].apply(lambda context: find_matching_source(context, dft))\n",
        "\n",
        "# Menandai kalimat yang tidak memiliki Pronouns sebagai \"Unambiguous\"\n",
        "pronoun_ambigu.loc[pronoun_ambigu['Pronouns'].isna(), 'Pronouns_Ambiguous'] = 'Unambiguous'\n",
        "\n",
        "# Mengacak urutan baris pada DataFrame 'pronoun_ambigu'\n",
        "pronoun_ambigu = pronoun_ambigu.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Menampilkan DataFrame 'pronoun_ambigu' setelah pemrosesan\n",
        "pronoun_ambigu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFpRu12gRxoB"
      },
      "outputs": [],
      "source": [
        "pronoun_ambigu = pronoun_ambigu.reindex(columns=['Id', 'Source', 'Context', 'Pronouns', 'Antecedent', 'Pronouns_Ambiguous'])\n",
        "pronoun_ambigu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7d-mvI3n9K9"
      },
      "source": [
        "#Deteksi (Kata Sifat ambigu, Kata Keterangan ambigu) dan Frasa Komparatif"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eS2IaUioND3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3E-qVAT7ogrA"
      },
      "outputs": [],
      "source": [
        "data = pronoun_ambigu\n",
        "data = data.drop(columns='Id')\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJvsnRNkqVcz"
      },
      "outputs": [],
      "source": [
        "# Load model bahasa Inggris dari spaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Fungsi untuk menandai part-of-speech dan mendeteksi ambiguitas\n",
        "def mark_pos_and_detect_ambiguity(text):\n",
        "    doc = nlp(text)\n",
        "    marked_text = \"\"\n",
        "\n",
        "    for i, token in enumerate(doc):\n",
        "        if token.pos_ == \"ADJ\" and token.dep_ == 'amod':\n",
        "            marked_text += f\"[{token.text}] \"\n",
        "\n",
        "        elif token.pos_ == \"ADV\":\n",
        "            marked_text += f\"[{token.text}] \"\n",
        "\n",
        "        else:\n",
        "            if '#1' in token.text:\n",
        "                marked_text += token.text.replace('#1', '') + \" \"\n",
        "            else:\n",
        "                marked_text += token.text + \" \"\n",
        "    return marked_text.strip()\n",
        "\n",
        "# Tambahkan kolom-kolom baru ke DataFrame\n",
        "data[\"Marked_Text\"] = data['Context'].apply(mark_pos_and_detect_ambiguity)\n",
        "\n",
        "# Simpan hasilnya kembali ke CSV jika diperlukan\n",
        "data.to_csv(\"data_hasil.csv\", index=False)  # Gantilah \"data_hasil.csv\" sesuai kebutuhan Anda\n",
        "\n",
        "# Baca kembali data hasil\n",
        "data_hasil = pd.read_csv('data_hasil.csv', encoding='latin1')\n",
        "data_hasil\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2OfkEZ5S-pG"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Fungsi untuk memeriksa apakah ada dependensi 'nummod'\n",
        "def ambil_kalimat_dalam_kurung_siku(teks):\n",
        "    return re.findall(r'\\[([^\\]]+)\\]', teks)\n",
        "\n",
        "def generate_interpretation(text):\n",
        "    doc = nlp(text)\n",
        "    interpretation = []\n",
        "\n",
        "    for token in doc:\n",
        "        # Lemmatisasi token\n",
        "        kata_dasar = token.lemma_\n",
        "        interpretation.append((kata_dasar, token.dep_, token.head.text))\n",
        "\n",
        "    return interpretation\n",
        "\n",
        "def check_nummod_dependency(text):\n",
        "    doc = nlp(text)\n",
        "    kata_dalam_kurung_siku = ambil_kalimat_dalam_kurung_siku(text)\n",
        "\n",
        "    if not kata_dalam_kurung_siku:\n",
        "        return 'Unambiguous'\n",
        "\n",
        "    interpretation = generate_interpretation(text)\n",
        "    for token in doc:\n",
        "        if token.dep_ == 'nummod':\n",
        "            return 'Unambiguous'\n",
        "\n",
        "    return 'Ambiguous'\n",
        "\n",
        "# Tambahkan kolom baru ke DataFrame\n",
        "data_hasil['ADJ_ADV_Ambiguous'] = data_hasil['Marked_Text'].apply(check_nummod_dependency)\n",
        "data_hasil\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7pf-MCZdFbz3"
      },
      "outputs": [],
      "source": [
        "# Load model bahasa Inggris dari spaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Fungsi untuk menandai part-of-speech dan mendeteksi ambiguitas\n",
        "def mark_pos_and_detect_ambiguity(text):\n",
        "    doc = nlp(text)\n",
        "    comparative_phrases = []\n",
        "\n",
        "    for i, token in enumerate(doc):\n",
        "        if i > 0 and doc[i-1].text.lower() in [\"more\", \"less\", ]:\n",
        "            # Cek apakah token sebelumnya adalah \"more\" atau \"less\"\n",
        "            comparative_phrases.append(f\"{doc[i-1].text} {token.text}\")\n",
        "\n",
        "        elif token.pos_ == \"ADJ\" and token.text.endswith(\"er\") and token.text != 'other':\n",
        "            # Cek apakah kata sifat berakhiran \"er\"\n",
        "            comparative_phrases.append(token.text)\n",
        "\n",
        "    return comparative_phrases\n",
        "\n",
        "# Tambahkan kolom-kolom baru ke DataFrame\n",
        "data_hasil[\"Comparative_Phrases\"] = data_hasil['Context'].apply(mark_pos_and_detect_ambiguity)\n",
        "data_hasil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4PEunu4T7nF"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "\n",
        "# Load model bahasa Inggris dari spaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Fungsi untuk menandai part-of-speech dan mendeteksi ambiguitas\n",
        "def mark_pos_and_detect_ambiguity(text):\n",
        "    doc = nlp(text)\n",
        "    comparative_phrases = []\n",
        "\n",
        "    for i, token in enumerate(doc):\n",
        "        if i > 0 and doc[i-1].text.lower() in [\"more\", \"less\"]:\n",
        "            # Cek apakah token sebelumnya adalah \"more\" atau \"less\"\n",
        "            comparative_phrases.append(f\"{doc[i-1].text} {token.text}\")\n",
        "\n",
        "        elif token.pos_ == \"ADJ\" and token.text.endswith(\"er\") and token.text != 'other':\n",
        "            # Cek apakah kata sifat berakhiran \"er\"\n",
        "            comparative_phrases.append(token.text)\n",
        "\n",
        "    if len(comparative_phrases) > 0:\n",
        "        # Jika terdapat comparative phrase, tetapi cek apakah ada nummod\n",
        "        has_nummod = any([token.dep_ == 'nummod' and token.pos_ == 'NUM' for token in doc])\n",
        "        if has_nummod:\n",
        "            return 'Unambiguous'\n",
        "        else:\n",
        "            return 'Ambiguous'\n",
        "    else:\n",
        "        return 'Unambiguous'\n",
        "\n",
        "# Tambahkan kolom baru ke DataFrame untuk label\n",
        "data_hasil['Comparative_Phrase_Ambiguous'] = data_hasil['Context'].apply(mark_pos_and_detect_ambiguity)\n",
        "data_hasil\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUkfXBNjPJZm"
      },
      "outputs": [],
      "source": [
        "# Fungsi untuk mengganti nilai 'Ambiguous' dengan 1 dan 'Unambiguous' dengan 0\n",
        "def replace_labels(value):\n",
        "    if value == 'Ambiguous':\n",
        "        return 1\n",
        "    elif value == 'Unambiguous':\n",
        "        return 0\n",
        "    else:\n",
        "        return value\n",
        "\n",
        "# Mengaplikasikan fungsi\n",
        "data_hasil['Pronouns_Ambiguous'] = data_hasil['Pronouns_Ambiguous'].apply(replace_labels)\n",
        "data_hasil['ADJ_ADV_Ambiguous'] = data_hasil['ADJ_ADV_Ambiguous'].apply(replace_labels)\n",
        "data_hasil['Comparative_Phrase_Ambiguous'] = data_hasil['Comparative_Phrase_Ambiguous'].apply(replace_labels)\n",
        "data_hasil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8EZwhV7jQSF5"
      },
      "outputs": [],
      "source": [
        "# Fungsi untuk memberi label berdasarkan kondisi yang disebutkan\n",
        "def label_ambiguous(row):\n",
        "    if row['Pronouns_Ambiguous'] == 1 or row['ADJ_ADV_Ambiguous'] == 1 or row['Comparative_Phrase_Ambiguous'] == 1:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# Menambahkan kolom baru 'Label'\n",
        "data_hasil['Label'] = data_hasil.apply(label_ambiguous, axis=1)\n",
        "\n",
        "# Menampilkan data setelah penambahan kolom baru\n",
        "print(data_hasil[['Pronouns_Ambiguous', 'ADJ_ADV_Ambiguous', 'Comparative_Phrase_Ambiguous', 'Label']])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAytyESVcb0k"
      },
      "source": [
        "# Analisis Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ho3LYsb0kop6"
      },
      "source": [
        "## Analisis frekuensi istilah ambigu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7Q8bX6qk1u3"
      },
      "source": [
        "### DAMIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7I-kNMBek7PI"
      },
      "outputs": [],
      "source": [
        "# Filter dataset untuk mencakup hanya entri 'Source' dan memiliki label ambigu pada kolom Istilah Ambigu\n",
        "ambiguous_damir_data = data_hasil[(data_hasil['Source'] == 'DAMIR') & (data_hasil['Pronouns_Ambiguous'] == 1)]\n",
        "ambiguous_damir_data_2 = data_hasil[(data_hasil['Source'] == 'DAMIR') & (data_hasil['ADJ_ADV_Ambiguous'] == 1)]\n",
        "ambiguous_damir_data_3 = data_hasil[(data_hasil['Source'] == 'DAMIR') & (data_hasil['Comparative_Phrase_Ambiguous'] == 1)]\n",
        "ambiguous_label_damir = data_hasil[(data_hasil['Source'] == 'DAMIR') & (data_hasil['Label'] == 1)]\n",
        "\n",
        "# Hitung jumlah baris yang memiliki 'Source' DAMIR dan label ambigu pada kolom 'Pronouns_Ambiguous'\n",
        "total_ambiguous_damir = ambiguous_damir_data.shape[0]\n",
        "total_ambiguous_damir_2 = ambiguous_damir_data_2.shape[0]\n",
        "total_ambiguous_damir_3 = ambiguous_damir_data_3.shape[0]\n",
        "total_ambiguous_label_damir = ambiguous_label_damir.shape[0]\n",
        "\n",
        "# Hitung total baris yang memiliki 'Source' DAMIR\n",
        "total_damir_data = data_hasil[data_hasil['Source'] == 'DAMIR'].shape[0]\n",
        "\n",
        "# Hitung persentase 'Context' yang memiliki label ambigu pada dataset yang memiliki 'Source' DAMIR\n",
        "percentage_ambiguous_damir = (total_ambiguous_damir / total_damir_data) * 100\n",
        "percentage_ambiguous_damir_2 = (total_ambiguous_damir_2 / total_damir_data) * 100\n",
        "percentage_ambiguous_damir_3 = (total_ambiguous_damir_3 / total_damir_data) * 100\n",
        "\n",
        "print(f\"DAMIR (Kata Ganti Ambigu): {percentage_ambiguous_damir:.2f}%\")\n",
        "print(f\"DAMIR (Kata Sifat & Keterangan Ambigu): {percentage_ambiguous_damir_2:.2f}%\")\n",
        "print(f\"DAMIR (Frasa Komparatif Ambigu): {percentage_ambiguous_damir_3:.2f}%\")\n",
        "print(\"Total requirements Ambigu pada dataset DAMIR :\", total_ambiguous_label_damir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pBOJyvqk_HG"
      },
      "source": [
        "###DOSSPRE\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKoZzcsulGNv"
      },
      "outputs": [],
      "source": [
        "# Filter dataset untuk mencakup hanya entri 'Source' dan memiliki label ambigu pada kolom Istilah Ambigu\n",
        "ambiguous_data_dosspre = data_hasil[(data_hasil['Source'] == 'DOSSPRE') & (data_hasil['Pronouns_Ambiguous'] == 1)]\n",
        "ambiguous_data_2_dosspre = data_hasil[(data_hasil['Source'] == 'DOSSPRE') & (data_hasil['ADJ_ADV_Ambiguous'] == 1)]\n",
        "ambiguous_data_3_dosspre = data_hasil[(data_hasil['Source'] == 'DOSSPRE') & (data_hasil['Comparative_Phrase_Ambiguous'] == 1)]\n",
        "ambiguous_dosspre_label = data_hasil[(data_hasil['Source'] == 'DOSSPRE') & (data_hasil['Label'] == 1)]\n",
        "\n",
        "# Hitung jumlah baris yang memiliki 'Source' DAMIR dan label ambigu pada kolom 'Pronouns_Ambiguous'\n",
        "total_ambiguous_dosspre = ambiguous_data_dosspre.shape[0]\n",
        "total_ambiguous_2_dosspre = ambiguous_data_2_dosspre.shape[0]\n",
        "total_ambiguous_3_dosspre = ambiguous_data_3_dosspre.shape[0]\n",
        "total_ambiguous_label_dosspre = ambiguous_dosspre_label.shape[0]\n",
        "\n",
        "# Hitung total baris yang memiliki 'Source' DAMIR\n",
        "total_data_dosspre = data_hasil[data_hasil['Source'] == 'DOSSPRE'].shape[0]\n",
        "\n",
        "# Hitung persentase 'Context' yang memiliki label ambigu pada dataset yang memiliki 'Source' DAMIR\n",
        "percentage_ambiguous = (total_ambiguous_dosspre / total_data_dosspre) * 100\n",
        "percentage_ambiguous_2 = (total_ambiguous_2_dosspre / total_data_dosspre) * 100\n",
        "percentage_ambiguous_3 = (total_ambiguous_3_dosspre / total_data_dosspre) * 100\n",
        "\n",
        "print(f\"DOSSPRE (Kata Ganti Ambigu): {percentage_ambiguous:.2f}%\")\n",
        "print(f\"DOSSPRE (Kata Sifat & Keterangan Ambigu): {percentage_ambiguous_2:.2f}%\")\n",
        "print(f\"DOSSPRE (Frasa Komparatif Ambigu): {percentage_ambiguous_3:.2f}%\")\n",
        "print(\"Total requirements Ambigu pada dataset DOSSPRE :\", total_ambiguous_label_dosspre)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsXr_jo2lKct"
      },
      "source": [
        "### Type Requirement (Kaggle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pa5Gvdf7lRc-"
      },
      "outputs": [],
      "source": [
        "# Filter dataset untuk mencakup hanya entri 'Source' dan memiliki label ambigu pada kolom Istilah Ambigu\n",
        "ambiguous_data = data_hasil[(data_hasil['Source'] == 'KAGGLETYPE') & (data_hasil['Pronouns_Ambiguous'] == 1)]\n",
        "ambiguous_data_2 = data_hasil[(data_hasil['Source'] == 'KAGGLETYPE') & (data_hasil['ADJ_ADV_Ambiguous'] == 1)]\n",
        "ambiguous_data_3 = data_hasil[(data_hasil['Source'] == 'KAGGLETYPE') & (data_hasil['Comparative_Phrase_Ambiguous'] == 1)]\n",
        "ambiguous_type_label = data_hasil[(data_hasil['Source'] == 'KAGGLETYPE') & (data_hasil['Label'] == 1)]\n",
        "\n",
        "\n",
        "# Hitung jumlah baris yang memiliki 'Source' DAMIR dan label ambigu pada kolom 'Pronouns_Ambiguous'\n",
        "total_ambiguous = ambiguous_data.shape[0]\n",
        "total_ambiguous_2 = ambiguous_data_2.shape[0]\n",
        "total_ambiguous_3 = ambiguous_data_3.shape[0]\n",
        "total_ambiguous_label_type = ambiguous_type_label.shape[0]\n",
        "\n",
        "# Hitung total baris yang memiliki 'Source' DAMIR\n",
        "total_data_kaggle = data_hasil[data_hasil['Source'] == 'KAGGLETYPE'].shape[0]\n",
        "\n",
        "# Hitung persentase 'Context' yang memiliki label ambigu pada dataset yang memiliki 'Source' DAMIR\n",
        "percentage_ambiguous = (total_ambiguous / total_data_kaggle) * 100\n",
        "percentage_ambiguous_2 = (total_ambiguous_2 / total_data_kaggle) * 100\n",
        "percentage_ambiguous_3 = (total_ambiguous_3 / total_data_kaggle) * 100\n",
        "\n",
        "print(f\"KAGGLETYPE (Kata Ganti Ambigu): {percentage_ambiguous:.2f}%\")\n",
        "print(f\"KAGGLETYPE (Kata Sifat & Keterangan Ambigu): {percentage_ambiguous_2:.2f}%\")\n",
        "print(f\"KAGGLETYPE (Frasa Komparatif Ambigu): {percentage_ambiguous_3:.2f}%\")\n",
        "print(\"Total requirements Ambigu pada dataset Kaggle :\", total_ambiguous_label_type)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hH0ZjXn5lZC_"
      },
      "source": [
        "### Seluruh Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUm6gkpnlelL"
      },
      "outputs": [],
      "source": [
        "percentage_pronouns_ambiguous = (data_hasil['Pronouns_Ambiguous'].sum() / len(data_hasil)) * 100\n",
        "\n",
        "percentage_adj_adv_ambiguous = (data_hasil['ADJ_ADV_Ambiguous'].sum() / len(data_hasil)) * 100\n",
        "\n",
        "percentage_comparative_phrase_ambiguous = (data_hasil['Comparative_Phrase_Ambiguous'].sum() / len(data_hasil)) * 100\n",
        "\n",
        "print(f\"Presentase Pronouns yang berlabel ambigu: {percentage_pronouns_ambiguous:.2f}%\")\n",
        "print(f\"Presentase ADJ_ADV yang berlabel ambigu: {percentage_adj_adv_ambiguous:.2f}%\")\n",
        "print(f\"Presentase Comparative Phrase yang berlabel ambigu: {percentage_comparative_phrase_ambiguous:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVEYoD2Vlnsg"
      },
      "source": [
        "## Analisis kombinasi kemungkinan ambigu pada dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U984SW29cRjb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Kombinasi nilai yang akan dihitung\n",
        "combinations = {\n",
        "    '111': ['Pronouns_Ambiguous', 'ADJ_ADV_Ambiguous', 'Comparative_Phrase_Ambiguous'],\n",
        "    '100': ['Pronouns_Ambiguous', 'ADJ_ADV_Ambiguous', 'Comparative_Phrase_Ambiguous'],\n",
        "    '010': ['Pronouns_Ambiguous', 'ADJ_ADV_Ambiguous', 'Comparative_Phrase_Ambiguous'],\n",
        "    '001': ['Pronouns_Ambiguous', 'ADJ_ADV_Ambiguous', 'Comparative_Phrase_Ambiguous'],\n",
        "    '110': ['Pronouns_Ambiguous', 'ADJ_ADV_Ambiguous', 'Comparative_Phrase_Ambiguous'],\n",
        "    '011': ['Pronouns_Ambiguous', 'ADJ_ADV_Ambiguous', 'Comparative_Phrase_Ambiguous'],\n",
        "    '101': ['Pronouns_Ambiguous', 'ADJ_ADV_Ambiguous', 'Comparative_Phrase_Ambiguous'],\n",
        "    '000': ['Pronouns_Ambiguous', 'ADJ_ADV_Ambiguous', 'Comparative_Phrase_Ambiguous']\n",
        "}\n",
        "\n",
        "# Inisialisasi variabel untuk menyimpan jumlah kombinasi\n",
        "combination_counts = {}\n",
        "\n",
        "# Menghitung jumlah kombinasi nilai\n",
        "for key, values in combinations.items():\n",
        "    query = ' & '.join(f\"{value} == {int(key[index])}\" for index, value in enumerate(values))\n",
        "    combination_counts[key] = data_hasil.query(query).shape[0]\n",
        "\n",
        "# Menghitung total data\n",
        "total_data = data_hasil.shape[0]\n",
        "\n",
        "# Menghitung presentase setiap kombinasi nilai\n",
        "percentage_combinations = {key: round((value / total_data) * 100, 2) for key, value in combination_counts.items()}\n",
        "\n",
        "# Menyimpan hasil presentase ke dalam DataFrame pandas\n",
        "df = pd.DataFrame(list(percentage_combinations.items()), columns=['Kombinasi', 'Presentase'])\n",
        "\n",
        "# Menampilkan tabel hasil presentase\n",
        "df\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EDShDJqmfmV"
      },
      "source": [
        "## Analisis tipe requirement yang kemungkinan besar terdapat istilah ambigu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmBGg1o8TYP4"
      },
      "outputs": [],
      "source": [
        "# Fungsi untuk mengecek apakah kata kunci ada dalam teks\n",
        "def check_keywords(text):\n",
        "    keywords = [\n",
        "        'performance', 'space', 'time', 'throughput', 'response', 'memory', 'consumption', 'fast', 'index', 'triggers',\n",
        "        'storage', 'low', 'run', 'runtime', 'perform', 'execute', 'mean', 'peak', 'compress', 'dynamic', 'offset',\n",
        "        'reduce', 'fixing', 'early', 'processing', 'security', 'confidentiality', 'integrity', 'availability',\n",
        "        'accuracy', 'completeness', 'secure', 'access', 'registration', 'authorization', 'identification',\n",
        "        'authentication', 'validation', 'transaction', 'user', 'password', 'control', 'encryption', 'key', 'spoofing',\n",
        "        'attack', 'policy', 'logging', 'permission', 'Utility', 'Homogeneity', 'Easiness', 'Operationability',\n",
        "        'intuitively', 'adaptable', 'Understandability', 'accessible', 'Configuration', 'Integrity', 'Administration',\n",
        "        'Conformity', 'cognizance', 'applicable', 'Linguistics', 'supportive', 'tutorials', 'trainable', 'helping',\n",
        "        'flexible', 'easy', 'usable', 'Graphics', 'timing'\n",
        "    ]\n",
        "    for keyword in keywords:\n",
        "        if keyword.lower() in text.lower():\n",
        "            return 'Non-Functional'\n",
        "    return 'Functional'\n",
        "\n",
        "# Menambahkan kolom baru 'Label' berdasarkan analisis menggunakan fungsi check_keywords\n",
        "data_hasil['Type'] = data_hasil['Context'].apply(check_keywords)\n",
        "\n",
        "data_hasil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEULWJoQUCqN"
      },
      "outputs": [],
      "source": [
        "ambiguous_pronouns_functional = data_hasil[(data_hasil['Pronouns_Ambiguous'] == 1) & (data_hasil['Type'] == 'Functional')]\n",
        "ambiguous_pronouns_non_functional = data_hasil[(data_hasil['Pronouns_Ambiguous'] == 1) & (data_hasil['Type'] == 'Non-Functional')]\n",
        "\n",
        "total_ambiguous_pronouns_functional = ambiguous_pronouns_functional.shape[0]\n",
        "total_ambiguous_pronouns_non_functional = ambiguous_pronouns_non_functional.shape[0]\n",
        "\n",
        "# Hitung total baris yang memiliki nilai ambigu pada pronouns\n",
        "total_data_pronouns_ambiguous = data_hasil[data_hasil['Pronouns_Ambiguous'] == 1].shape[0]\n",
        "\n",
        "percentage_ambiguous_functional = (total_ambiguous_pronouns_functional / total_data_pronouns_ambiguous) * 100\n",
        "percentage_ambiguous_non_functional = (total_ambiguous_pronouns_non_functional / total_data_pronouns_ambiguous) * 100\n",
        "\n",
        "print(f\"Total Pronouns ambigu: {total_data_pronouns_ambiguous}\")\n",
        "print(f\"Functional (Pronouns Ambiguous): {percentage_ambiguous_functional:.2f}%\")\n",
        "print(f\"Non-Functional (Pronouns Ambiguous): {percentage_ambiguous_non_functional:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAsVads2ZlCR"
      },
      "outputs": [],
      "source": [
        "ambiguous_adj_functional = data_hasil[(data_hasil['ADJ_ADV_Ambiguous'] == 1) & (data_hasil['Type'] == 'Functional')]\n",
        "ambiguous_adj_non_functional = data_hasil[(data_hasil['ADJ_ADV_Ambiguous'] == 1) & (data_hasil['Type'] == 'Non-Functional')]\n",
        "\n",
        "total_ambiguous_adj_functional = ambiguous_adj_functional.shape[0]\n",
        "total_ambiguous_adj_non_functional = ambiguous_adj_non_functional.shape[0]\n",
        "\n",
        "# Hitung total baris yang memiliki nilai ambigu pada pronouns\n",
        "total_data_adj_ambiguous = data_hasil[data_hasil['ADJ_ADV_Ambiguous'] == 1].shape[0]\n",
        "\n",
        "percentage_ambiguous_functional_adj = (total_ambiguous_adj_functional / total_data_adj_ambiguous) * 100\n",
        "percentage_ambiguous_non_functional_adj = (total_ambiguous_adj_non_functional / total_data_adj_ambiguous) * 100\n",
        "\n",
        "print(f\"Total ADJ_ADV ambigu: {total_data_adj_ambiguous}\")\n",
        "print(f\"Functional (ADJ_ADV Ambiguous): {percentage_ambiguous_functional_adj:.2f}%\")\n",
        "print(f\"Non-Functional (ADJ_ADV Ambiguous): {percentage_ambiguous_non_functional_adj:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXSk4BMVa7u2"
      },
      "outputs": [],
      "source": [
        "ambiguous_com_functional = data_hasil[(data_hasil['Comparative_Phrase_Ambiguous'] == 1) & (data_hasil['Type'] == 'Functional')]\n",
        "ambiguous_com_non_functional = data_hasil[(data_hasil['Comparative_Phrase_Ambiguous'] == 1) & (data_hasil['Type'] == 'Non-Functional')]\n",
        "\n",
        "total_ambiguous_com_functional = ambiguous_com_functional.shape[0]\n",
        "total_ambiguous_com_non_functional = ambiguous_com_non_functional.shape[0]\n",
        "\n",
        "total_data_com_ambiguous = data_hasil[data_hasil['Comparative_Phrase_Ambiguous'] == 1].shape[0]\n",
        "\n",
        "percentage_ambiguous_functional_com = (total_ambiguous_com_functional / total_data_com_ambiguous) * 100\n",
        "percentage_ambiguous_non_functional_com = (total_ambiguous_com_non_functional / total_data_com_ambiguous) * 100\n",
        "\n",
        "print(f\"Total Comparative ambigu: {total_data_com_ambiguous}\")\n",
        "print(f\"Functional (Comparative Ambiguous): {percentage_ambiguous_functional_com:.2f}%\")\n",
        "print(f\"Non-Functional (Comparative Ambiguous): {percentage_ambiguous_non_functional_com:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ambiguous_label_functional = data_hasil[(data_hasil['Label'] == 1) & (data_hasil['Type'] == 'Functional')]\n",
        "ambiguous_label_non_functional = data_hasil[(data_hasil['Label'] == 1) & (data_hasil['Type'] == 'Non-Functional')]\n",
        "\n",
        "total_ambiguous_label_functional = ambiguous_label_functional.shape[0]\n",
        "total_ambiguous_label_non_functional = ambiguous_label_non_functional.shape[0]\n",
        "\n",
        "total_data_label_ambiguous = data_hasil[data_hasil['Label'] == 1].shape[0]\n",
        "\n",
        "percentage_ambiguous_functional_label = (total_ambiguous_label_functional / total_data_label_ambiguous) * 100\n",
        "percentage_ambiguous_non_functional_label = (total_ambiguous_label_non_functional / total_data_label_ambiguous) * 100\n",
        "\n",
        "print(f\"Total Label ambigu: {total_data_label_ambiguous}\")\n",
        "print(f\"Functional (Label Ambiguous): {percentage_ambiguous_functional_label:.2f}%\")\n",
        "print(f\"Non-Functional (ADJ_ADV Ambiguous): {percentage_ambiguous_non_functional_label:.2f}%\")"
      ],
      "metadata": {
        "id": "O0l9U1PEtaEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8ckfV4ATwo6"
      },
      "source": [
        "## Analisis istilah (Shall, Will dan Should)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hbfL4eiT3oR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Fungsi untuk mencari kata shall, will, should dalam teks\n",
        "def find_auxiliary_verbs(text):\n",
        "    # Menggunakan ekspresi reguler untuk mencari kata shall, will, should dalam teks\n",
        "    aux_verbs = re.findall(r'\\b(?:shall|will|should)\\b', text)\n",
        "    return ', '.join(aux_verbs)\n",
        "\n",
        "# Membuat kolom baru untuk kata kunci shall, will, should\n",
        "data_hasil['Auxiliary_Verbs'] = data_hasil['Context'].apply(find_auxiliary_verbs)\n",
        "\n",
        "# Menampilkan DataFrame dengan kolom baru\n",
        "data_hasil\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ynDVSdzZEra"
      },
      "source": [
        "### Shall\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gguXir35ZCzN"
      },
      "outputs": [],
      "source": [
        "# Menghitung jumlah persyaratan yang memenuhi kriteria\n",
        "shall_pronouns_ambiguous = data_hasil[\n",
        "    (data_hasil['Auxiliary_Verbs'].str.contains('shall', na=False)) &\n",
        "    (data_hasil['Pronouns_Ambiguous'] == 1)\n",
        "].shape[0]\n",
        "\n",
        "shall_adj_adv_ambiguous = data_hasil[\n",
        "    (data_hasil['Auxiliary_Verbs'].str.contains('shall', na=False)) &\n",
        "    (data_hasil['ADJ_ADV_Ambiguous'] == 1)\n",
        "].shape[0]\n",
        "\n",
        "shall_comparative_ambiguous = data_hasil[\n",
        "    (data_hasil['Auxiliary_Verbs'].str.contains('shall', na=False)) &\n",
        "    (data_hasil['Comparative_Phrase_Ambiguous'] == 1)\n",
        "].shape[0]\n",
        "\n",
        "# Menghitung jumlah total persyaratan yang memiliki kata 'shall' di Auxiliary_Verbs\n",
        "total_requirements_with_shall = data_hasil['Auxiliary_Verbs'].str.contains('shall', na=False).sum()\n",
        "\n",
        "# Menghitung presentase\n",
        "percentage_shall_pronouns_ambiguous = (shall_pronouns_ambiguous / total_requirements_with_shall) * 100\n",
        "percentage_shall_adj_adv_ambiguous = (shall_adj_adv_ambiguous / total_requirements_with_shall) * 100\n",
        "percentage_shall_comparative_ambiguous = (shall_comparative_ambiguous / total_requirements_with_shall) * 100\n",
        "\n",
        "# Menampilkan hasil\n",
        "print(f\"Persentase persyaratan dengan kata 'shall' dan Pronouns_Ambiguous label 1: {percentage_shall_pronouns_ambiguous:.2f}%\")\n",
        "print(f\"Persentase persyaratan dengan kata 'shall' dan ADJ_ADV_Ambiguous label 1: {percentage_shall_adj_adv_ambiguous:.2f}%\")\n",
        "print(f\"Persentase persyaratan dengan kata 'shall' dan Comparative_Phrase_Ambiguous label 1: {percentage_shall_comparative_ambiguous:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Menghitung jumlah requirement Non-Functional yang menggunakan kata shall\n",
        "non_func_shall_count = data_hasil[(data_hasil['Type'] == 'Non-Functional') & (data_hasil['Auxiliary_Verbs'].str.contains('shall'))].shape[0]\n",
        "\n",
        "# Menghitung jumlah requirement Functional yang menggunakan kata shall\n",
        "func_shall_count = data_hasil[(data_hasil['Type'] == 'Functional') & (data_hasil['Auxiliary_Verbs'].str.contains('shall'))].shape[0]\n",
        "\n",
        "# Menghitung total requirement Non-Functional\n",
        "total_non_func_count = data_hasil[data_hasil['Type'] == 'Non-Functional'].shape[0]\n",
        "\n",
        "# Menghitung total requirement Functional\n",
        "total_func_count = data_hasil[data_hasil['Type'] == 'Functional'].shape[0]\n",
        "\n",
        "# Menghitung presentase requirement Non-Functional yang menggunakan kata shall\n",
        "percentage_non_func_shall = (non_func_shall_count / total_non_func_count) * 100\n",
        "\n",
        "# Menghitung presentase requirement Functional yang menggunakan kata shall\n",
        "percentage_func_shall = (func_shall_count / total_func_count) * 100\n",
        "\n",
        "print(\"Total requirement yang menggunakan shall :\", total_requirements_with_shall )\n",
        "print(\"Presentase Requirement Non-Functional yang menggunakan kata 'shall': {:.2f}%\".format(percentage_non_func_shall))\n",
        "print(\"Presentase Requirement Functional yang menggunakan kata 'shall': {:.2f}%\".format(percentage_func_shall))\n"
      ],
      "metadata": {
        "id": "Q0974jTc9QdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_YFAkKfaLNc"
      },
      "source": [
        "### Will"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HiHiq8o8aOTD"
      },
      "outputs": [],
      "source": [
        "# Menghitung jumlah persyaratan yang memenuhi kriteria\n",
        "will_pronouns_ambiguous = data_hasil[\n",
        "    (data_hasil['Auxiliary_Verbs'].str.contains('will', na=False)) &\n",
        "    (data_hasil['Pronouns_Ambiguous'] == 1)\n",
        "].shape[0]\n",
        "\n",
        "will_adj_adv_ambiguous = data_hasil[\n",
        "    (data_hasil['Auxiliary_Verbs'].str.contains('will', na=False)) &\n",
        "    (data_hasil['ADJ_ADV_Ambiguous'] == 1)\n",
        "].shape[0]\n",
        "\n",
        "will_comparative_ambiguous = data_hasil[\n",
        "    (data_hasil['Auxiliary_Verbs'].str.contains('will', na=False)) &\n",
        "    (data_hasil['Comparative_Phrase_Ambiguous'] == 1)\n",
        "].shape[0]\n",
        "\n",
        "# Menghitung jumlah total persyaratan yang memiliki kata 'will' di Auxiliary_Verbs\n",
        "total_requirements_with_will = data_hasil['Auxiliary_Verbs'].str.contains('will', na=False).sum()\n",
        "\n",
        "# Menghitung presentase\n",
        "percentage_will_pronouns_ambiguous = (will_pronouns_ambiguous / total_requirements_with_will) * 100\n",
        "percentage_will_adj_adv_ambiguous = (will_adj_adv_ambiguous / total_requirements_with_will) * 100\n",
        "percentage_will_comparative_ambiguous = (will_comparative_ambiguous / total_requirements_with_will) * 100\n",
        "\n",
        "# Menampilkan hasil\n",
        "print(f\"Persentase persyaratan dengan kata 'will' dan Pronouns_Ambiguous label 1: {percentage_will_pronouns_ambiguous:.2f}%\")\n",
        "print(f\"Persentase persyaratan dengan kata 'will' dan ADJ_ADV_Ambiguous label 1: {percentage_will_adj_adv_ambiguous:.2f}%\")\n",
        "print(f\"Persentase persyaratan dengan kata 'will' dan Comparative_Phrase_Ambiguous label 1: {percentage_will_comparative_ambiguous:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Menghitung jumlah requirement Non-Functional yang menggunakan kata will\n",
        "non_func_will_count = data_hasil[(data_hasil['Type'] == 'Non-Functional') & (data_hasil['Auxiliary_Verbs'].str.contains('will'))].shape[0]\n",
        "\n",
        "# Menghitung jumlah requirement Functional yang menggunakan kata will\n",
        "func_will_count = data_hasil[(data_hasil['Type'] == 'Functional') & (data_hasil['Auxiliary_Verbs'].str.contains('will'))].shape[0]\n",
        "\n",
        "# Menghitung total requirement Non-Functional\n",
        "total_non_func_count = data_hasil[data_hasil['Type'] == 'Non-Functional'].shape[0]\n",
        "\n",
        "# Menghitung total requirement Functional\n",
        "total_func_count = data_hasil[data_hasil['Type'] == 'Functional'].shape[0]\n",
        "\n",
        "# Menghitung presentase requirement Non-Functional yang menggunakan kata will\n",
        "percentage_non_func_will = (non_func_will_count / total_non_func_count) * 100\n",
        "\n",
        "# Menghitung presentase requirement Functional yang menggunakan kata will\n",
        "percentage_func_will = (func_will_count / total_func_count) * 100\n",
        "\n",
        "print(\"Total requirement yang menggunakan will :\", total_requirements_with_will )\n",
        "print(\"Presentase Requirement Non-Functional yang menggunakan kata 'will': {:.2f}%\".format(percentage_non_func_will))\n",
        "print(\"Presentase Requirement Functional yang menggunakan kata 'will': {:.2f}%\".format(percentage_func_will))\n"
      ],
      "metadata": {
        "id": "Pqe81KC09sMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LeuCJBuc18V"
      },
      "source": [
        "### Should"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkdaW1A-c5jb"
      },
      "outputs": [],
      "source": [
        "# Menghitung jumlah persyaratan yang memenuhi kriteria\n",
        "should_pronouns_ambiguous = data_hasil[\n",
        "    (data_hasil['Auxiliary_Verbs'].str.contains('should', na=False)) &\n",
        "    (data_hasil['Pronouns_Ambiguous'] == 1)\n",
        "].shape[0]\n",
        "\n",
        "should_adj_adv_ambiguous = data_hasil[\n",
        "    (data_hasil['Auxiliary_Verbs'].str.contains('should', na=False)) &\n",
        "    (data_hasil['ADJ_ADV_Ambiguous'] == 1)\n",
        "].shape[0]\n",
        "\n",
        "should_comparative_ambiguous = data_hasil[\n",
        "    (data_hasil['Auxiliary_Verbs'].str.contains('should', na=False)) &\n",
        "    (data_hasil['Comparative_Phrase_Ambiguous'] == 1)\n",
        "].shape[0]\n",
        "\n",
        "# Menghitung jumlah total persyaratan yang memiliki kata 'should' di Auxiliary_Verbs\n",
        "total_requirements_with_should = data_hasil['Auxiliary_Verbs'].str.contains('should', na=False).sum()\n",
        "\n",
        "# Menghitung presentase\n",
        "percentage_should_pronouns_ambiguous = (should_pronouns_ambiguous / total_requirements_with_should) * 100\n",
        "percentage_should_adj_adv_ambiguous = (should_adj_adv_ambiguous / total_requirements_with_should) * 100\n",
        "percentage_should_comparative_ambiguous = (should_comparative_ambiguous / total_requirements_with_should) * 100\n",
        "\n",
        "# Menampilkan hasil\n",
        "print(f\"Persentase persyaratan dengan kata 'should' dan Pronouns_Ambiguous label 1: {percentage_should_pronouns_ambiguous:.2f}%\")\n",
        "print(f\"Persentase persyaratan dengan kata 'should' dan ADJ_ADV_Ambiguous label 1: {percentage_should_adj_adv_ambiguous:.2f}%\")\n",
        "print(f\"Persentase persyaratan dengan kata 'should' dan Comparative_Phrase_Ambiguous label 1: {percentage_should_comparative_ambiguous:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tGbNAimKCkG"
      },
      "outputs": [],
      "source": [
        "# Menghitung jumlah requirement Non-Functional yang menggunakan kata should\n",
        "non_func_should_count = data_hasil[(data_hasil['Type'] == 'Non-Functional') & (data_hasil['Auxiliary_Verbs'].str.contains('should'))].shape[0]\n",
        "\n",
        "# Menghitung jumlah requirement Functional yang menggunakan kata should\n",
        "func_should_count = data_hasil[(data_hasil['Type'] == 'Functional') & (data_hasil['Auxiliary_Verbs'].str.contains('should'))].shape[0]\n",
        "\n",
        "# Menghitung total requirement Non-Functional\n",
        "total_non_func_count = data_hasil[data_hasil['Type'] == 'Non-Functional'].shape[0]\n",
        "\n",
        "# Menghitung total requirement Functional\n",
        "total_func_count = data_hasil[data_hasil['Type'] == 'Functional'].shape[0]\n",
        "\n",
        "# Menghitung presentase requirement Non-Functional yang menggunakan kata should\n",
        "percentage_non_func_should = (non_func_should_count / total_non_func_count) * 100\n",
        "\n",
        "# Menghitung presentase requirement Functional yang menggunakan kata should\n",
        "percentage_func_should = (func_should_count / total_func_count) * 100\n",
        "\n",
        "print(\"Total requirement yang menggunakan should :\", total_requirements_with_should )\n",
        "print(\"Presentase Requirement Non-Functional yang menggunakan kata 'should': {:.2f}%\".format(percentage_non_func_should))\n",
        "print(\"Presentase Requirement Functional yang menggunakan kata 'should': {:.2f}%\".format(percentage_func_should))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Menghitung jumlah requirement Non-Functional atau Functional\n",
        "count_requirement_type = data_hasil['Label'].value_counts()\n",
        "\n",
        "# Menampilkan hasil\n",
        "print(count_requirement_type)\n"
      ],
      "metadata": {
        "id": "RUBPBRsU-ywx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menghitung jumlah requirement yang memiliki label 1 dan menggunakan istilah \"shall\"\n",
        "shall_label_1_count = data_hasil[(data_hasil['Label'] == 1) & (data_hasil['Auxiliary_Verbs'].str.contains('shall', case=False))].shape[0]\n",
        "\n",
        "# Menghitung jumlah requirement yang memiliki label 1 dan menggunakan istilah \"will\"\n",
        "will_label_1_count = data_hasil[(data_hasil['Label'] == 1) & (data_hasil['Auxiliary_Verbs'].str.contains('will', case=False))].shape[0]\n",
        "\n",
        "# Menghitung jumlah requirement yang memiliki label 1 dan menggunakan istilah \"should\"\n",
        "should_label_1_count = data_hasil[(data_hasil['Label'] == 1) & (data_hasil['Auxiliary_Verbs'].str.contains('should', case=False))].shape[0]\n",
        "\n",
        "# Menghitung total requirement yang memiliki label 1\n",
        "total_label_1_count = data_hasil[data_hasil['Label'] == 1].shape[0]\n",
        "\n",
        "# Menghitung presentase requirement yang memiliki label 1 dan menggunakan istilah \"shall\"\n",
        "percentage_shall_label_1 = (shall_label_1_count / total_label_1_count) * 100\n",
        "\n",
        "# Menghitung presentase requirement yang memiliki label 1 dan menggunakan istilah \"will\"\n",
        "percentage_will_label_1 = (will_label_1_count / total_label_1_count) * 100\n",
        "\n",
        "# Menghitung presentase requirement yang memiliki label 1 dan menggunakan istilah \"should\"\n",
        "percentage_should_label_1 = (should_label_1_count / total_label_1_count) * 100\n",
        "\n",
        "print(f\"Presentase requirement yang memiliki label 1 dan menggunakan istilah 'shall': {percentage_shall_label_1:.2f}%\")\n",
        "print(f\"Presentase requirement yang memiliki label 1 dan menggunakan istilah 'will': {percentage_will_label_1:.2f}%\")\n",
        "print(f\"Presentase requirement yang memiliki label 1 dan menggunakan istilah 'should': {percentage_should_label_1:.2f}%\")\n"
      ],
      "metadata": {
        "id": "vNeNmnJgV21d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fungsi untuk menghitung jumlah requirement berlabel 1 yang menggunakan kata kunci shall, will, atau should\n",
        "def count_requirements_with_auxiliary_verbs(df):\n",
        "    # Filter requirements yang memiliki label 1 dan menggunakan kata kunci shall, will, atau should\n",
        "    filtered_df = df[(df['Label'] == 1) & (df['Auxiliary_Verbs'] != '')]\n",
        "\n",
        "    # Menghitung jumlah requirement yang memenuhi kriteria tersebut\n",
        "    count = len(filtered_df)\n",
        "\n",
        "    return count\n",
        "\n",
        "# Memanggil fungsi untuk menghitung jumlah requirement berlabel 1 dengan menggunakan kata kunci shall, will, atau should\n",
        "jumlah_requirement = count_requirements_with_auxiliary_verbs(data_hasil)\n",
        "\n",
        "# Menampilkan jumlah requirement yang memenuhi kriteria\n",
        "print(\"Jumlah requirement berlabel 1 yang menggunakan kata kunci shall, will, atau should:\", jumlah_requirement)\n"
      ],
      "metadata": {
        "id": "aY4Ydolah8j9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fungsi untuk menghitung presentase requirement berlabel 1 yang menggunakan kata kunci shall, will, atau should\n",
        "def calculate_percentage_requirements_with_auxiliary_verbs(df):\n",
        "    # Total jumlah requirement berlabel 1\n",
        "    total_requirement_label_1 = len(df[df['Label'] == 1])\n",
        "\n",
        "    # Jumlah requirement berlabel 1 yang menggunakan kata kunci shall, will, atau should\n",
        "    count_requirement_with_auxiliary_verbs = count_requirements_with_auxiliary_verbs(df)\n",
        "\n",
        "    # Menghitung persentase\n",
        "    percentage = (count_requirement_with_auxiliary_verbs / total_requirement_label_1) * 100\n",
        "\n",
        "    return percentage\n",
        "\n",
        "# Memanggil fungsi untuk menghitung presentase requirement berlabel 1 yang menggunakan kata kunci shall, will, atau should\n",
        "presentase_requirement = calculate_percentage_requirements_with_auxiliary_verbs(data_hasil)\n",
        "\n",
        "# Menampilkan presentase\n",
        "print(\"Presentase requirement berlabel 1 yang menggunakan kata kunci shall, will, atau should:\", presentase_requirement, \"%\")\n"
      ],
      "metadata": {
        "id": "iigsXeBjiS61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menghitung total jumlah requirement berlabel 1\n",
        "total_label_1 = data_hasil[data_hasil['Label'] == 1].shape[0]\n",
        "\n",
        "# Menghitung jumlah requirement berlabel 1 yang tidak menggunakan istilah shall, will, atau should\n",
        "count_non_aux_verbs = data_hasil[(data_hasil['Label'] == 1) & (~data_hasil['Auxiliary_Verbs'].str.contains(r'shall|will|should', case=False, na=False))].shape[0]\n",
        "\n",
        "# Menghitung presentasenya\n",
        "percentage = (count_non_aux_verbs / total_label_1) * 100\n",
        "\n",
        "print(\"Presentase requirement berlabel 1 yang tidak menggunakan istilah shall, will, atau should:\", percentage, \"%\")\n"
      ],
      "metadata": {
        "id": "LsX3-SPBZwjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menghitung jumlah requirement berlabel 1 yang tidak menggunakan istilah shall, will, atau should\n",
        "count_non_aux_verbs = data_hasil[(data_hasil['Label'] == 1) & (~data_hasil['Auxiliary_Verbs'].str.contains(r'shall|will|should', case=False, na=False))].shape[0]\n",
        "\n",
        "print(\"Jumlah requirement berlabel 1 yang tidak menggunakan istilah shall, will, atau should:\", count_non_aux_verbs)\n"
      ],
      "metadata": {
        "id": "kTCKrud6aUEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fungsi untuk menghitung jumlah requirement berlabel 1 yang menggunakan kata kunci shall, will, atau should\n",
        "def count_requirements_with_auxiliary_verbs(df):\n",
        "    # Filter requirements yang memiliki label 1 dan menggunakan kata kunci shall, will, atau should\n",
        "    filtered_df = df[(df['Label'] == 0) & (df['Auxiliary_Verbs'] != '')]\n",
        "\n",
        "    # Menghitung jumlah requirement yang memenuhi kriteria tersebut\n",
        "    count = len(filtered_df)\n",
        "\n",
        "    return count\n",
        "\n",
        "# Memanggil fungsi untuk menghitung jumlah requirement berlabel 1 dengan menggunakan kata kunci shall, will, atau should\n",
        "jumlah_requirement = count_requirements_with_auxiliary_verbs(data_hasil)\n",
        "\n",
        "# Menampilkan jumlah requirement yang memenuhi kriteria\n",
        "print(\"Jumlah requirement berlabel 0 yang menggunakan kata kunci shall, will, atau should:\", jumlah_requirement)\n"
      ],
      "metadata": {
        "id": "flJHJlDdimxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fungsi untuk menghitung presentase requirement berlabel 1 yang menggunakan kata kunci shall, will, atau should\n",
        "def calculate_percentage_requirements_with_auxiliary_verbs(df):\n",
        "    # Total jumlah requirement berlabel 1\n",
        "    total_requirement_label_1 = len(df[df['Label'] == 0])\n",
        "\n",
        "    # Jumlah requirement berlabel 1 yang menggunakan kata kunci shall, will, atau should\n",
        "    count_requirement_with_auxiliary_verbs = count_requirements_with_auxiliary_verbs(df)\n",
        "\n",
        "    # Menghitung persentase\n",
        "    percentage = (count_requirement_with_auxiliary_verbs / total_requirement_label_1) * 100\n",
        "\n",
        "    return percentage\n",
        "\n",
        "# Memanggil fungsi untuk menghitung presentase requirement berlabel 1 yang menggunakan kata kunci shall, will, atau should\n",
        "presentase_requirement = calculate_percentage_requirements_with_auxiliary_verbs(data_hasil)\n",
        "\n",
        "# Menampilkan presentase\n",
        "print(\"Presentase requirement berlabel 0 yang menggunakan kata kunci shall, will, atau should:\", presentase_requirement, \"%\")\n"
      ],
      "metadata": {
        "id": "5mhTEvaGiwcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menghitung total jumlah requirement berlabel 1\n",
        "total_label_1 = data_hasil[data_hasil['Label'] == 0].shape[0]\n",
        "\n",
        "# Menghitung jumlah requirement berlabel 1 yang tidak menggunakan istilah shall, will, atau should\n",
        "count_non_aux_verbs = data_hasil[(data_hasil['Label'] == 0) & (~data_hasil['Auxiliary_Verbs'].str.contains(r'shall|will|should', case=False, na=False))].shape[0]\n",
        "\n",
        "# Menghitung presentasenya\n",
        "percentage = (count_non_aux_verbs / total_label_1) * 100\n",
        "\n",
        "print(\"Presentase requirement berlabel 0 yang tidak menggunakan istilah shall, will, atau should:\", percentage, \"%\")\n"
      ],
      "metadata": {
        "id": "SQ8RMH4pi6iC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menghitung jumlah requirement berlabel 1 yang tidak menggunakan istilah shall, will, atau should\n",
        "count_non_aux_verbs = data_hasil[(data_hasil['Label'] == 0) & (~data_hasil['Auxiliary_Verbs'].str.contains(r'shall|will|should', case=False, na=False))].shape[0]\n",
        "\n",
        "print(\"Jumlah requirement berlabel 1 yang tidak menggunakan istilah shall, will, atau should:\", count_non_aux_verbs)\n"
      ],
      "metadata": {
        "id": "ZRWzJhcyjH5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_label_1(dataframe):\n",
        "    return dataframe[dataframe['Label'] == 0]['Label'].count()\n",
        "\n",
        "jumlah_label_1 = count_label_1(data_hasil)\n",
        "print(\"Jumlah requirement dengan label 1:\", jumlah_label_1)\n"
      ],
      "metadata": {
        "id": "FZ3HvwwreSjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4KF-9MAlugY"
      },
      "source": [
        "## Analisis perkembangan frekuensi istilah ambigu pertahun"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BartGr3l3d6"
      },
      "source": [
        "### 2022\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaZEZULGIhV5"
      },
      "outputs": [],
      "source": [
        "# Memilih subset dari data yang memiliki Source DAMIR atau DOSSPRE\n",
        "subset_data = pronoun_ambigu[(pronoun_ambigu['Source'] == 'DAMIR') | (pronoun_ambigu['Source'] == 'DOSSPRE')]\n",
        "\n",
        "# Menghitung jumlah total konteks pada subset data\n",
        "total_contexts = len(subset_data)\n",
        "\n",
        "# Menghitung jumlah konteks yang memiliki label ambigu pada subset data\n",
        "total_ambiguous_damir = ambiguous_damir_data.shape[0]\n",
        "total_ambiguous_damir_2 = ambiguous_damir_data_2.shape[0]\n",
        "total_ambiguous_damir_3 = ambiguous_damir_data_3.shape[0]\n",
        "\n",
        "\n",
        "total_ambiguous_dosspre = ambiguous_data_dosspre.shape[0]\n",
        "total_ambiguous_2_dosspre = ambiguous_data_2_dosspre.shape[0]\n",
        "total_ambiguous_3_dosspre = ambiguous_data_3_dosspre.shape[0]\n",
        "\n",
        "ambiguous_contexts_1 = total_ambiguous_damir + total_ambiguous_dosspre\n",
        "ambiguous_contexts_2 = total_ambiguous_damir_2 + total_ambiguous_2_dosspre\n",
        "ambiguous_contexts_3 = total_ambiguous_damir_3 + total_ambiguous_3_dosspre\n",
        "\n",
        "\n",
        "# Menghitung persentase konteks yang memiliki label ambigu pada subset data\n",
        "percentage_ambiguous = (ambiguous_contexts_1 / total_contexts) * 100\n",
        "percentage_ambiguous_2 = (ambiguous_contexts_2 / total_contexts) * 100\n",
        "percentage_ambiguous_3 = (ambiguous_contexts_3 / total_contexts) * 100\n",
        "\n",
        "# Menampilkan hasil\n",
        "print(f\"2022 (Kata Ganti Ambigu): {percentage_ambiguous:.2f}%\")\n",
        "print(f\"2022 (Kata Sifat & Keterangan Ambigu): {percentage_ambiguous_2:.2f}%\")\n",
        "print(f\"2022 (Frasa Komparatif): {percentage_ambiguous_3:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Et3Bj6cyl6Mh"
      },
      "source": [
        "### 2023"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKLl-JOPlWWx"
      },
      "outputs": [],
      "source": [
        "# Filter dataset untuk mencakup hanya entri 'Source' dan memiliki label ambigu pada kolom Istilah Ambigu\n",
        "ambiguous_data = data_hasil[(data_hasil['Source'] == 'KAGGLETYPE') & (data_hasil['Pronouns_Ambiguous'] == 1)]\n",
        "ambiguous_data_2 = data_hasil[(data_hasil['Source'] == 'KAGGLETYPE') & (data_hasil['ADJ_ADV_Ambiguous'] == 1)]\n",
        "ambiguous_data_3 = data_hasil[(data_hasil['Source'] == 'KAGGLETYPE') & (data_hasil['Comparative_Phrase_Ambiguous'] == 1)]\n",
        "\n",
        "# Hitung jumlah baris yang memiliki 'Source' DAMIR dan label ambigu pada kolom 'Pronouns_Ambiguous'\n",
        "total_ambiguous = ambiguous_data.shape[0]\n",
        "total_ambiguous_2 = ambiguous_data_2.shape[0]\n",
        "total_ambiguous_3 = ambiguous_data_3.shape[0]\n",
        "\n",
        "# Hitung total baris yang memiliki 'Source' DAMIR\n",
        "total_data_kaggle = data_hasil[data_hasil['Source'] == 'KAGGLETYPE'].shape[0]\n",
        "\n",
        "# Hitung persentase 'Context' yang memiliki label ambigu pada dataset yang memiliki 'Source' DAMIR\n",
        "percentage_ambiguous = (total_ambiguous / total_data_kaggle) * 100\n",
        "percentage_ambiguous_2 = (total_ambiguous_2 / total_data_kaggle) * 100\n",
        "percentage_ambiguous_3 = (total_ambiguous_3 / total_data_kaggle) * 100\n",
        "\n",
        "print\n",
        "print(f\"KAGGLETYPE (Kata Ganti Ambigu): {percentage_ambiguous:.2f}%\")\n",
        "print(f\"KAGGLETYPE (Kata Sifat & Keterangan Ambigu): {percentage_ambiguous_2:.2f}%\")\n",
        "print(f\"KAGGLETYPE (Frasa Komparatif Ambigu): {percentage_ambiguous_3:.2f}%\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyORfO4M1VtU4N9TS2np2sdM",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}